{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNiANxXnU6yoLHJ7vUv/20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilylusvardii/Final_Project_Code/blob/main/practical_work_w21010751_KV6003.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade tf_keras\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
        "!pip show tensorflow transformers tf_keras"
      ],
      "metadata": {
        "id": "4kLeqlptVMjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "!python -m spacy download it_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "KJOUFXfJTDXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pre trained BERT model fine tuned on dataset, unit testing used here."
      ],
      "metadata": {
        "id": "SbomWp-d8oSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "import unittest\n",
        "import sys\n",
        "\n",
        "#unit testing\n",
        "class unitTesting(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.ds = pd.DataFrame({\n",
        "            'text': ['Awful experience!!!', 'Molto Bello.', 'It was okay, I Guess?'],\n",
        "            'rating': [1, 5, 3]\n",
        "        })\n",
        "    @staticmethod\n",
        "    def toSent(rating):\n",
        "        if rating <= 2:\n",
        "            return 0  #negative\n",
        "        elif rating == 3:\n",
        "            return 1  #neutral\n",
        "        else:\n",
        "            return 2  #positive\n",
        "\n",
        "    def preprocessing(self, train_texts, test_texts, train_labels, test_labels, tokeniser=None):\n",
        "        if tokeniser:\n",
        "            MAX_LENGTH = 512\n",
        "            train_encodings = tokeniser(train_texts.tolist(), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "            test_encodings = tokeniser(test_texts.tolist(), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "            train_labels = tf.convert_to_tensor(train_labels.tolist())\n",
        "            test_labels = tf.convert_to_tensor(test_labels.tolist())\n",
        "            return train_encodings, test_encodings, train_labels, test_labels\n",
        "        else:\n",
        "            return train_texts, test_texts, train_labels, test_labels\n",
        "\n",
        "    def test_mapping(self):\n",
        "        self.assertEqual(unitTesting.toSent(1), 0)  #negative\n",
        "        self.assertEqual(unitTesting.toSent(5), 2)  #positive\n",
        "        self.assertEqual(unitTesting.toSent(3), 1)  #neutral\n",
        "\n",
        "    def test_preprocess(self):\n",
        "        self.ds['sentiment'] = self.ds['rating'].apply(unitTesting.toSent)\n",
        "        train_texts, test_texts, train_labels, test_labels = train_test_split(self.ds['text'], self.ds['sentiment'], test_size=0.3)\n",
        "        train_texts, test_texts, train_labels, test_labels = self.preprocessing(train_texts, test_texts, train_labels, test_labels)\n",
        "        self.assertEqual(len(train_texts), 2)\n",
        "        self.assertEqual(len(test_texts), 1)\n",
        "\n",
        "    def test_model(self):\n",
        "        self.ds['sentiment'] = self.ds['rating'].apply(unitTesting.toSent)\n",
        "        train_texts, test_texts, train_labels, test_labels = train_test_split(self.ds['text'], self.ds['sentiment'], test_size=0.3)\n",
        "        tokeniser = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        train_encodings, test_encodings, train_labels, test_labels = self.preprocessing(train_texts, test_texts, train_labels, test_labels, tokeniser=tokeniser)\n",
        "        classes = 3\n",
        "        BERTmodel = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=classes)\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        metrics = ['accuracy']\n",
        "        BERTmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "        trained = BERTmodel.fit(\n",
        "            {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
        "            train_labels,\n",
        "            validation_data=(\n",
        "                {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},\n",
        "                test_labels\n",
        "            ),\n",
        "            batch_size=32,\n",
        "            epochs=3\n",
        "        )\n",
        "        self.assertIsNotNone(trained)\n",
        "\n",
        "\n",
        "def main_model():\n",
        "    start = time.time() #timing training time\n",
        "    print(\"started timer!\")\n",
        "\n",
        "    #loading dataset\n",
        "    ds = pd.read_csv('reviews.csv', encoding='utf-8')\n",
        "    print(\"dataset loaded!\")\n",
        "    #mapping ratings to sentiments\n",
        "    ds['sentiment'] = ds['rating'].apply(unitTesting.toSent)\n",
        "    print(\"sentiments mapped!\")\n",
        "\n",
        "    #data split into training/testing 70/30\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(ds['text'], ds['sentiment'], test_size=0.3, random_state=42)\n",
        "\n",
        "    #loading bert tokeniser\n",
        "    tokeniser = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    #preprocessing\n",
        "    MAX_LENGTH = 512\n",
        "    train_encodings = tokeniser(train_texts.tolist(), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "    test_encodings = tokeniser(test_texts.tolist(), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "    #converting to tensor\n",
        "    train_labels = tf.convert_to_tensor(train_labels.tolist())\n",
        "    test_labels = tf.convert_to_tensor(test_labels.tolist())\n",
        "\n",
        "    #loading bert model\n",
        "    classes = 3\n",
        "    BERTmodel = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=classes)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metrics = ['accuracy']\n",
        "    BERTmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    #training/finetuning model\n",
        "    BERTmodel.fit(\n",
        "        {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
        "        train_labels,\n",
        "        validation_data=(\n",
        "            {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},\n",
        "            test_labels\n",
        "        ),\n",
        "        batch_size=16,\n",
        "        epochs=3\n",
        "    )\n",
        "    print(\"training complete!\")\n",
        "\n",
        "    #making predictions\n",
        "    predictions = BERTmodel.predict(test_encodings)\n",
        "    print(\"predictions have been made!\")\n",
        "    # Printing evaluation metrics\n",
        "    print(classification_report(test_labels, predictions[0].argmax(axis=1)))\n",
        "    #confusion matrix\n",
        "    confusion = confusion_matrix(test_labels, predictions[0].argmax(axis=1))\n",
        "    plt.figure(figsize=(8,8))\n",
        "    sns.heatmap(confusion, annot=True, fmt=\"d\", cmap='PiYG', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
        "    plt.xlabel('predicted')\n",
        "    plt.ylabel('actual')\n",
        "    plt.title('confusion matrix')\n",
        "    plt.show()\n",
        "\n",
        "    end = time.time()  #ending training timer\n",
        "    print(f\"training time in secs= {end - start}\") #outputs in seconds\n",
        "\n",
        "    #saving model for future use using .keras\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive') #mounting google drive\n",
        "    #save path\n",
        "    filepath = '/content/drive/My Drive/DistilBERT_FineTuned.keras'\n",
        "    BERTmodel.save(filepath)\n",
        "    print(\"model saved successfully in google drive!\")\n",
        "\n",
        "#test already ran commented out code to run it\n",
        "if __name__ == '__main__':\n",
        "    #if len(sys.argv) > 1 and sys.argv[1] == 'run-main':\n",
        "        main_model()\n",
        "    #else:\n",
        "        #unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "nzM_NZUZw6Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM model trained and saved to google drive. Unit testing used here too."
      ],
      "metadata": {
        "id": "u7qEjHPy7aYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from langdetect import detect\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import save_model\n",
        "import unittest\n",
        "import sys\n",
        "\n",
        "\n",
        "#unit testing\n",
        "class unitTesting(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        #setup DataFrame with sample data\n",
        "        self.ds = pd.DataFrame({\n",
        "            'text': ['Awful experience!!!', 'Molto Bello.', 'It was okay, I Guess?'],\n",
        "            'rating': [1, 5, 3]\n",
        "        })\n",
        "\n",
        "        #loading spacy models\n",
        "        self.ENspacy = spacy.load('en_core_web_sm')\n",
        "        self.ITspacy = spacy.load('it_core_news_sm')\n",
        "\n",
        "        #applying preprocessing\n",
        "        self.ds['preprocessed'] = self.ds['text'].apply(self.preprocess)\n",
        "\n",
        "        #tokensing and padding\n",
        "        tokeniser = Tokenizer(num_words=5000)\n",
        "        tokeniser.fit_on_texts(self.ds['preprocessed'])\n",
        "        sequences = tokeniser.texts_to_sequences(self.ds['preprocessed'])\n",
        "        self.data = pad_sequences(sequences, maxlen=100)\n",
        "        self.labels = np.array([self.toSent(rating) for rating in self.ds['rating']])\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        try:\n",
        "            #detecting language and choosing model based on this\n",
        "            doc = self.ENspacy(text) if detect(text) == 'en' else self.ITspacy(text)\n",
        "            tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_stop]\n",
        "            #joining tokens back together\n",
        "            return ' '.join(tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"error with preprocessing: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def toSent(rating):\n",
        "        #mapping ratings to sentiments\n",
        "        if rating <= 2:\n",
        "            return 0  #negative\n",
        "        elif rating == 3:\n",
        "            return 1  #neutral\n",
        "        else:\n",
        "            return 2  #positive\n",
        "\n",
        "\n",
        "    def test_mapping(self):\n",
        "        #testing mapping function\n",
        "        self.assertEqual(self.toSent(1), 0)  #negative\n",
        "        self.assertEqual(self.toSent(5), 2)  #positive\n",
        "        self.assertEqual(self.toSent(3), 1)  #neutral\n",
        "\n",
        "    def test_preprocessing(self):\n",
        "        #testing preprocessing\n",
        "        expected_outputs = ['awful experience', 'bello', 'okay guess']\n",
        "        processed_texts = self.ds['preprocessed'].tolist()\n",
        "        self.assertEqual(processed_texts, expected_outputs)\n",
        "\n",
        "    def test_tokenisation(self):\n",
        "        #testing tokenisation\n",
        "        self.assertEqual(len(self.data), len(self.ds))  #data length check\n",
        "        self.assertEqual(self.data.shape[1], 100)  #sequence length check\n",
        "\n",
        "    def test_training(self):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(self.data, self.labels, test_size=0.3, random_state=42)\n",
        "        #building LSTM model\n",
        "        model = Sequential([\n",
        "            Embedding(input_dim=5000, output_dim=64, input_length=100),\n",
        "            LSTM(64),\n",
        "            Dense(3, activation='softmax')\n",
        "        ])\n",
        "        #compiling model\n",
        "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        #training model\n",
        "        history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=16, epochs=3)#smaller for testing\n",
        "        self.assertIsNotNone(history)  #making sure model has actually been trained\n",
        "        #testing if training ran sucessfully\n",
        "        results = model.evaluate(X_test, y_test)\n",
        "\n",
        "\n",
        "def main_model():\n",
        "        #using this time import to time how long training takes\n",
        "        start = time.time()\n",
        "        print (\"started timer!\")\n",
        "\n",
        "        #importing data\n",
        "        print (\"importing data from dataset!\")\n",
        "        ds = pd.read_csv('reviews.csv', encoding='utf-8')\n",
        "\n",
        "        #mapping ratings to sentiments\n",
        "        print (\"mapping sentiments\")\n",
        "        def toSent (rating):\n",
        "            if rating <= 2:\n",
        "                return 'negative'\n",
        "            if rating == 3:\n",
        "                return 'neutral'\n",
        "            else:\n",
        "                return 'positive'\n",
        "\n",
        "        #sentiment values now in new column\n",
        "        ds['sentiment'] = ds['rating'].apply(toSent)\n",
        "\n",
        "        #preprocesing for both english and italian - using spaCy\n",
        "        #loading the spacy models\n",
        "        ENspacy = spacy.load('en_core_web_sm')\n",
        "        ITspacy = spacy.load('it_core_news_sm')\n",
        "\n",
        "        print(\"preprocessing started!\")\n",
        "        def preprocess(text):\n",
        "            try:\n",
        "               #detecting language and choosing spacy model based off this\n",
        "               doc = ENspacy(text) if detect(text) == 'en' else ITspacy(text)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"error with preprocessing= {e}\")\n",
        "\n",
        "            tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_stop]\n",
        "            #joining the text back together\n",
        "            return ' '.join(tokens)\n",
        "\n",
        "        #applying the spacy preprocessing to dataset\n",
        "        ds['preprocessed'] = ds['text'].apply(preprocess)\n",
        "\n",
        "        #tokenising\n",
        "        tokeniser = Tokenizer(num_words=5000)\n",
        "        tokeniser.fit_on_texts(ds['preprocessed'])\n",
        "        sequences = tokeniser.texts_to_sequences(ds['preprocessed'])\n",
        "        data = pad_sequences(sequences, maxlen=100)\n",
        "\n",
        "\n",
        "        #coverting labels back to numeric values again for use in model\n",
        "        labels = ds['sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
        "\n",
        "        #splitting training/testing data 70/30\n",
        "        X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "\n",
        "        #one-hot encode labels\n",
        "        y_train = to_categorical(y_train, num_classes=3)\n",
        "        y_test = to_categorical(y_test, num_classes=3)\n",
        "\n",
        "        #building model\n",
        "        print(\"model is now training, nearly done!\")\n",
        "        model = Sequential([\n",
        "            Embedding(input_dim=5000, output_dim=128, input_length=100),\n",
        "            SpatialDropout1D(0.3),\n",
        "            LSTM(100, dropout=0.3, recurrent_dropout=0.3),\n",
        "            Dense(3, activation='softmax')\n",
        "        ])\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        #training model\n",
        "        model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "        #evaluation metrics for LSTM\n",
        "        print(\"evaluating model! Results soon...\")\n",
        "        y_pred = model.predict(X_test, batch_size=64)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        print(classification_report(np.argmax(y_test, axis=1), y_pred_classes,  labels=[0, 1, 2], target_names=['negative', 'neutral', 'positive']))#results outputted\n",
        "        #confusion matrix\n",
        "        confusion = confusion_matrix (np.argmax(y_test, axis=1), y_pred_classes)\n",
        "        plt.figure(figsize=(8,8))\n",
        "        sns.heatmap(confusion, annot=True, fmt=\"d\", cmap='PiYG', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
        "        plt.xlabel('predicted')\n",
        "        plt.ylabel('actual')\n",
        "        plt.title('confusion matrix')\n",
        "        plt.show()\n",
        "\n",
        "        end = time.time()#ending training timer\n",
        "        trainingTime = end - start #outputs in seconds\n",
        "        print(f\"training time in secs= {trainingTime}\")\n",
        "\n",
        "        #saving model for future use using .keras\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive') #mounting google drive\n",
        "        #save path\n",
        "        filepath = '/content/drive/My Drive/ML_LSTM.keras'\n",
        "        model.save(filepath)\n",
        "        print(\"model saved successfully in google drive!\")\n",
        "\n",
        "#test already ran commented out code to run it\n",
        "if __name__ == '__main__':\n",
        "    #if len(sys.argv) > 1 and sys.argv[1] == 'run-main':\n",
        "        main_model()\n",
        "    #else:\n",
        "        #unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "7NLXf_zIaqZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM model trained and saved to google drive. Unit testing used here."
      ],
      "metadata": {
        "id": "eLeeXDdP7Npj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import unittest\n",
        "import sys\n",
        "\n",
        "\n",
        "class unitTesting(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        #setup with sample data\n",
        "        self.ds = pd.DataFrame({\n",
        "            'text': ['Awful experience!!!', 'Molto Bello.', 'It was okay, I Guess?'],\n",
        "            'rating': [1, 5, 3]\n",
        "        })\n",
        "        #mapping ratings to sentiments\n",
        "        self.ds['sentiment'] = self.ds['rating'].apply(self.toSent)\n",
        "        #loading spacy models\n",
        "        self.ENspacy = spacy.load('en_core_web_sm')\n",
        "        self.ITspacy = spacy.load('it_core_news_sm')\n",
        "\n",
        "        #preprocesssing and feature extraction\n",
        "        self.ds['processedText'] = self.ds['text'].apply(self.preprocess)\n",
        "        self.vectorizer = TfidfVectorizer(max_features=3000)\n",
        "        self.features = self.vectorizer.fit_transform(self.ds['processedText'])\n",
        "        self.labels = self.ds['sentiment']\n",
        "\n",
        "    @staticmethod\n",
        "    def toSent(rating):\n",
        "        if rating <= 2:\n",
        "            return 'negative'\n",
        "        elif rating == 3:\n",
        "            return 'neutral'\n",
        "        else:\n",
        "            return 'positive'\n",
        "\n",
        "    def preprocess(self, text): #doing preprocessing on the test set of data\n",
        "         try:\n",
        "            doc = self.ENspacy(text) if detect(text) == 'en' else self.ITspacy(text)\n",
        "            return ' '.join([token.text.lower() for token in doc if not token.is_punct and not token.is_stop])\n",
        "         except Exception as e:\n",
        "            return f\"error with preprocessing= {str(e)}\"\n",
        "\n",
        "    def test_preprocessing(self): #preprocessing test\n",
        "        expected_outputs = ['awful experience', 'bello', 'okay guess']\n",
        "        processed_texts = self.ds['processedText'].tolist()\n",
        "        for processed, expected in zip(processed_texts, expected_outputs):\n",
        "            self.assertEqual(processed, expected, f\"expected {expected} but got {processed}\")\n",
        "        actual_features = self.features.shape[1]\n",
        "        expected_features = min(3000, len(self.vectorizer.get_feature_names_out()))\n",
        "        self.assertEqual(actual_features, expected_features, f\"expected {expected_features} features, but got {actual_features}\")\n",
        "\n",
        "    def test_training(self): #model training test\n",
        "        X_train, X_test, y_train, y_test = train_test_split(self.features, self.labels, test_size=0.3, random_state=42)\n",
        "        SVMmodel = SVC(kernel='linear', class_weight='balanced')\n",
        "        SVMmodel.fit(X_train, y_train)\n",
        "        predictions = SVMmodel.predict(X_test)\n",
        "        self.assertIsNotNone(predictions, \"model failed to make predictions\")\n",
        "\n",
        "\n",
        "def main_model():\n",
        "        #using this time import to time how long training takes\n",
        "        start = time.time()\n",
        "        print (\"started timer!\")\n",
        "\n",
        "        #importing data\n",
        "        print (\"importing dataset!\")\n",
        "        ds = pd.read_csv('reviews.csv', encoding='utf-8')\n",
        "\n",
        "        #mapping ratings to sentiments\n",
        "        print (\"mapping sentiments\")\n",
        "        def toSent (rating):\n",
        "            if rating <= 2:\n",
        "                return 'negative'\n",
        "            if rating == 3:\n",
        "                return 'neutral'\n",
        "            else:\n",
        "                return 'positive'\n",
        "\n",
        "        #sentiment values now in new column\n",
        "        ds['sentiment'] = ds['rating'].apply(toSent)\n",
        "\n",
        "        #preprocesing for both english and italian - using spaCy\n",
        "        #loading the spacy models\n",
        "        ENspacy = spacy.load('en_core_web_sm')\n",
        "        ITspacy = spacy.load('it_core_news_sm')\n",
        "\n",
        "        print(\"preprocessing started!\")\n",
        "        def preprocess(text):\n",
        "            try:\n",
        "               #detecting language and choosing spacy model based off this\n",
        "                doc = ENspacy(text) if detect(text) == 'en' else ITspacy(text)\n",
        "            except Exception as e:\n",
        "                print(f\"error with preprocessing= {e}\")\n",
        "                return '{e}'\n",
        "\n",
        "            #tokenisation and getting rid of stop words and punctuation\n",
        "            tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_stop]\n",
        "                #joining the text back together\n",
        "            return ' '.join(tokens)\n",
        "\n",
        "        print(\"applying preprocesing to dataset!\")\n",
        "        #applying preprocessing to the dataset\n",
        "        ds['processedText'] = ds['text'].apply(preprocess)\n",
        "\n",
        "        print (\"extracting feautres!\")\n",
        "        #feature extraction of preprocessed text\n",
        "        vectoriser = TfidfVectorizer(max_features=3000)\n",
        "        x = vectoriser.fit_transform(ds['processedText'])\n",
        "        y = ds['sentiment']\n",
        "\n",
        "        print(\"starting training! Nearly done\")\n",
        "        #splitting training/testing data 70/30\n",
        "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "        #training svm model - intially usedlinear kernel\n",
        "        SVMmodel = SVC(kernel='linear', class_weight='balanced')\n",
        "        SVMmodel.fit(X_train, y_train)\n",
        "        #forming predictions\n",
        "        y_pred = SVMmodel.predict(X_test)\n",
        "\n",
        "        #evaluation metrics - outputted\n",
        "        print(classification_report(y_test, y_pred, zero_division=0))#addition based on warnings in terminal\n",
        "        #confusion matrix\n",
        "        confusion = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(8,8))\n",
        "        sns.heatmap(confusion, annot=True, fmt=\"d\", cmap='PiYG', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
        "        plt.xlabel('predicted')\n",
        "        plt.ylabel('actual')\n",
        "        plt.title('confusion matrix')\n",
        "        plt.show()\n",
        "\n",
        "        end = time.time()#ending training timer\n",
        "        trainingTime = end - start #outputs in seconds\n",
        "        print(f\"training time in secs= {trainingTime}\")\n",
        "\n",
        "        #saving model for future use using pickle\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        filepath='/content/drive/My Drive/ML_SVM'\n",
        "        pickle.dump(SVMmodel, open(filepath, 'wb'))\n",
        "        print(\"the model has been saved sucessfully\")\n",
        "\n",
        "#test already ran commented out code to run it\n",
        "if __name__ == '__main__':\n",
        "    #if len(sys.argv) > 1 and sys.argv[1] == 'run-main':\n",
        "        main_model()\n",
        "    #else:\n",
        "        #unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
      ],
      "metadata": {
        "id": "ASIrq2fxcfZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading SVM model on sample of dataset - for demo"
      ],
      "metadata": {
        "id": "bBaCiM3gI6n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from langdetect import detect\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import drive\n",
        "#mounting googel drive\n",
        "drive.mount('/content/drive')\n",
        "#loading saved SVM model\n",
        "modelpath = '/content/drive/My Drive/deliverables_w21010751_KV6003/ML_SVM'\n",
        "with open(modelpath, 'rb') as file:\n",
        "    SVMmodel = pickle.load(file)\n",
        "print(\"model loaded sucessfully!\")\n",
        "#importing data\n",
        "print (\"importing sample of dataset!\")\n",
        "ds = pd.read_csv('reviews.csv', encoding='utf-8')\n",
        "sample_size = 1000 #size of sample dataset\n",
        "\n",
        "#mapping ratings to sentiments\n",
        "print (\"mapping sentiments\")\n",
        "def toSent (rating):\n",
        "    if rating <= 2:\n",
        "        return 'negative'\n",
        "    if rating == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'positive'\n",
        "\n",
        "#sentiment values now in new column\n",
        "ds['sentiment'] = ds['rating'].apply(toSent)\n",
        "demo_data = ds.groupby('sentiment').apply(lambda x: x.sample(int(sample_size/len(ds['sentiment'].unique())))).reset_index(drop=True)\n",
        "\n",
        "#preprocesing for both english and italian - using spaCy\n",
        "#loading the spacy models\n",
        "ENspacy = spacy.load('en_core_web_sm')\n",
        "ITspacy = spacy.load('it_core_news_sm')\n",
        "\n",
        "print(\"preprocessing started!\")\n",
        "def preprocess(text):\n",
        "    try:\n",
        "        #detecting language and choosing spacy model based off this\n",
        "        doc = ENspacy(text) if detect(text) == 'en' else ITspacy(text)\n",
        "    except Exception as e:\n",
        "        print(f\"error with preprocessing= {e}\")\n",
        "        return '{e}'\n",
        "\n",
        "    #tokenisation and getting rid of stop words and punctuation\n",
        "    tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_stop]\n",
        "        #joining the text back together\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"applying preprocesing to dataset!\")\n",
        "#applying preprocessing to the dataset\n",
        "demo_data['processedText'] = demo_data['text'].apply(preprocess)\n",
        "\n",
        "print (\"extracting feautres!\")\n",
        "#feature extraction of preprocessed text\n",
        "vectoriser = TfidfVectorizer(max_features=3000)\n",
        "x = vectoriser.fit_transform(demo_data['processedText'])\n",
        "y = demo_data['sentiment']\n",
        "\n",
        "predictions = SVMmodel.predict(x)\n",
        "demo_data['predictions'] = predictions\n",
        "#classification report on the loaded model\n",
        "print (classification_report(demo_data['sentiment'], demo_data['predictions']))"
      ],
      "metadata": {
        "id": "FzU8LNKvI6KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading LSTM model on sample of dataset - for demo"
      ],
      "metadata": {
        "id": "eQz33D8TeOuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from langdetect import detect\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import drive\n",
        "#mounting google drive\n",
        "drive.mount('/content/drive')\n",
        "#loading saved LSTM model\n",
        "modelpath = '/content/drive/My Drive/deliverables_w21010751_KV6003/ML_LSTM.keras'\n",
        "LSTMmodel = load_model(modelpath)\n",
        "print(\"model loaded successfully!\")\n",
        "#importing data\n",
        "print (\"importing sample of dataset!\")\n",
        "ds = pd.read_csv('reviews.csv', encoding='utf-8')\n",
        "sample_size = 1000 #size of sample dataset\n",
        "\n",
        "#mapping ratings to sentiments\n",
        "print (\"mapping sentiments\")\n",
        "def toSent (rating):\n",
        "    if rating <= 2:\n",
        "        return 'negative'\n",
        "    if rating == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'positive'\n",
        "\n",
        "#sentiment values now in new column\n",
        "ds['sentiment'] = ds['rating'].apply(toSent)\n",
        "\n",
        "demo_data = ds.groupby('sentiment').apply(lambda x: x.sample(int(sample_size/len(ds['sentiment'].unique())))).reset_index(drop=True)\n",
        "#preprocesing for both english and italian - using spaCy\n",
        "#loading the spacy models\n",
        "ENspacy = spacy.load('en_core_web_sm')\n",
        "ITspacy = spacy.load('it_core_news_sm')\n",
        "\n",
        "print(\"preprocessing started!\")\n",
        "def preprocess(text):\n",
        "    try:\n",
        "        #detecting language and choosing spacy model based off this\n",
        "        doc = ENspacy(text) if detect(text) == 'en' else ITspacy(text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"error with preprocessing= {e}\")\n",
        "\n",
        "    tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_stop]\n",
        "    #joining the text back together\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "#applying the spacy preprocessing to dataset\n",
        "demo_data['preprocessed'] = demo_data['text'].apply(preprocess)\n",
        "\n",
        "#tokenising\n",
        "tokeniser = Tokenizer(num_words=5000)\n",
        "tokeniser.fit_on_texts(demo_data['preprocessed'])\n",
        "sequences = tokeniser.texts_to_sequences(demo_data['preprocessed'])\n",
        "data = pad_sequences(sequences, maxlen=100)\n",
        "\n",
        "\n",
        "#coverting labels back to numeric values again for use in model\n",
        "labels = demo_data['sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
        "\n",
        "#making predictions\n",
        "predictions = LSTMmodel.predict(data)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "#classification report\n",
        "print(classification_report(labels, predicted_classes, target_names=['negative', 'neutral', 'positive']))\n"
      ],
      "metadata": {
        "id": "yuM-C2gieN_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b365cd5-2b1f-4160-d0a2-62c50ca1d2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "model loaded successfully!\n",
            "importing sample of dataset!\n",
            "mapping sentiments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessing started!\n",
            "32/32 [==============================] - 12s 46ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.35      0.34      0.34       333\n",
            "     neutral       0.35      0.37      0.36       333\n",
            "    positive       0.28      0.27      0.28       333\n",
            "\n",
            "    accuracy                           0.33       999\n",
            "   macro avg       0.33      0.33      0.33       999\n",
            "weighted avg       0.33      0.33      0.33       999\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading DistilBERT model on sample of dataset - for demo"
      ],
      "metadata": {
        "id": "sG3dyls7tDMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import drive\n",
        "\n",
        "#mounting google drive\n",
        "drive.mount('/content/drive')\n",
        "#loading saved BERT model\n",
        "modelpath = '/content/drive/My Drive/deliverables_w21010751_KV6003/DistilBERT_FineTuned.keras'\n",
        "BERTmodel = load_model(modelpath)\n",
        "print(\"model loaded successfully!\")\n",
        "\n",
        "#importing data\n",
        "print(\"importing sample of dataset!\")\n",
        "ds = pd.read_csv('reviews.csv', encoding='utf-8')\n",
        "sample_size = 1000\n",
        "\n",
        "#mapping ratings to sentiments\n",
        "print(\"mapping sentiments\")\n",
        "def toSent(rating):\n",
        "    if rating <= 2:\n",
        "        return 'negative'\n",
        "    if rating == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'positive'\n",
        "\n",
        "ds['sentiment'] = ds['rating'].apply(toSent)\n",
        "#sampling data evenly so equal negative, neutral, positive reviews in small sample\n",
        "demo_data = ds.groupby('sentiment').apply(lambda x: x.sample(int(sample_size/len(ds['sentiment'].unique())))).reset_index(drop=True)\n",
        "#splitting into test train\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(demo_data['text'], demo_data['sentiment'], test_size=0.3, random_state=42)\n",
        "\n",
        "#encoding labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "test_labels = label_encoder.transform(test_labels)\n",
        "\n",
        "#loading tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "#preprocessing\n",
        "MAX_LENGTH = 512\n",
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "\n",
        "#converting to tensors\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels))\n",
        "\n",
        "#compiling model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = ['accuracy']\n",
        "BERTmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "#making predictions\n",
        "predictions = BERTmodel.predict(test_encodings)\n",
        "predicted_classes = np.argmax(predictions.logits, axis=1)\n",
        "print(classification_report(test_labels, predicted_classes))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GUdW01k5tC3K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}